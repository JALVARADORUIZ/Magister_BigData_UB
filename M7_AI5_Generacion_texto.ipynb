{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M7_AI5_Generacion_texto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JALVARADORUIZ/Magister_BigData_UB/blob/main/M7_AI5_Generacion_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*PADAWAN*: \n",
        "\n",
        "\n",
        "##JOSE ARMANDO ALVARADO RUIZ\n",
        "\n",
        "\n",
        "**Entrega: GENERACIÓN DE TEXTO**\n",
        "\n",
        "\n",
        "# **Descripción de la tarea**\n",
        "\n",
        "###Contexto\n",
        "\n",
        "En esta actividad utilizaremos una Red Neuronal Recurrente para generar texto. Esta es una funcionalidad básica para chatbots, asistentes de voz, etc.\n",
        "\n",
        "Se sugiere utilizar el fichero The-Collected-Works-of-HP-Lovecraft que recopila obras del escritor H.P. Lovecraft. Está formado por 700.000 palabras, de las cuales 36.000 son únicas.\n",
        "\n",
        "Alternativamente, puedes utilizar cualquier otro corpus que consideres.\n",
        "\n",
        "Para realizar la actividad, te pedimos lo siguiente: \n",
        "\n",
        "- Carga del dataset.\n",
        "- Preprocesado: utilizar la función “preprocess_corpus_if_necessary” del Colab proporcionado.\n",
        "- Generar una Red Neuronal Recurrente para generar texto según el estilo del dataset: \n",
        "  - El primer nivel será de embeddings únicamente para reducir la dimensionalidad.\n",
        "  - A continuación se añadirá una capa LSTM sencilla.\n",
        "  - A la salida de la capa LSTM se insertará una capa lineal.\n",
        "  - Esta capa lineal tendrá tantas salidas como palabras haya en el vocabulario.\n",
        "- Entrenar el modelo: \n",
        "  - Se generará un dataset compuesto por frases, y cada frase se obtiene escogiendo aleatoriamente del corpus una secuencia de palabras de longitud determinada y utilizando la palabra siguiente como etiqueta.\n",
        "- Evaluación del modelo: \n",
        "  - Grafica las curvas de loss y accuracy. \n",
        "  - Comenta los resultados. \n",
        "  - Genera textos y comprueba si son coherentes y se parecen al estilo del corpus. Utiliza la función “generate_texts” del Colab proporcionado. \n",
        "\n",
        "Consideraciones:\n",
        "- Framework a utilizar: Tensorflow.\n",
        "- Recuerda activar la GPU en el menú “Entorno de ejecución” => “Cambiar tipo de entorno de ejecución”.\n",
        "- Conviene revisar que las dimensiones de la Red son correctas antes de comenzar a entrenar."
      ],
      "metadata": {
        "id": "Wag0tV-Gqb1M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgmzVhKhmJHM"
      },
      "source": [
        "# Generación de texto con una red LSTM\n",
        "En esta actividad vamos a utilizar una red recurrente LSTM para generar texto.\n",
        "\n",
        "El corpus que sirve de referencia para generar los textos se compone de obras de H.P. Lovecraft, aunque se puede utilizar cualquier otro.\n",
        "\n",
        "El interés de esta actividad es utilizar las redes **LSTM para algo distinto de una clasificación**, aunque, como se puede observar, la calidad de los textos generados es inferior a la que se consigue con otros modelos más actuales, como los transformer.\n",
        "\n",
        "**Debes completar el código en las secciones indicadas con # COMPLETAR CODIGO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUGDPYoH6Vmh"
      },
      "source": [
        "## Instalación e importación de paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkmvTyPxA9bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7686c7-769a-4b05-e795-f9fbb292c60d"
      },
      "source": [
        "!pip install progressbar2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (3.38.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2) (3.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg_-Lc8WBBiu",
        "outputId": "2c2671c3-0555-4607-955d-04babb2ce83f"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import html\n",
        "import requests\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import pickle\n",
        "import random\n",
        "import progressbar\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize.moses import MosesDetokenizer\n",
        "    detokenizer = MosesDetokenizer()\n",
        "    use_moses_detokenizer = True\n",
        "except:\n",
        "    use_moses_detokenizer = False\n",
        "\n",
        "print(f\"use_moses_detokenizer={use_moses_detokenizer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use_moses_detokenizer=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WtteF7Mu9r-"
      },
      "source": [
        "## Definición de parámetros del corpus y su preprocesado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhtm7a7aCIr_"
      },
      "source": [
        "corpus_url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
        "corpus_path = \"pg11.txt\"\n",
        "preprocessed_corpus_path = \"alicia_preprocessed.p\"\n",
        "most_common_words_number = 10000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH8wS9yCvRXc"
      },
      "source": [
        "## Tranformación entre palabras e índices\n",
        "Se definen dos funciones:\n",
        "* **encode_sequence**: codifica una secuencia de palabras en una secuencia de índices\n",
        "* **decode_indices**: decodifica una secuencia de índices en la secuencia de palabras correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uqjM9GSKNxM"
      },
      "source": [
        "def encode_sequence(sequence, vocabulary):\n",
        "\n",
        "    return [vocabulary.index(element) for element in sequence if element in vocabulary]\n",
        "\n",
        "\n",
        "def decode_indices(indices, vocabulary):\n",
        "\n",
        "    decoded_tokens = [vocabulary[index] for index in indices]\n",
        "    if use_moses_detokenizer  == True:\n",
        "        return detokenizer.detokenize(decoded_tokens, return_str=True)\n",
        "    else:\n",
        "        return \" \".join(decoded_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0YFmoL9hXHi"
      },
      "source": [
        "## Descarga del corpus\n",
        "Solo se descarga si no se encuentra en local (Colab o disco)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1NGiz_3hUru",
        "outputId": "162381c9-cdd7-4c22-8d93-38bbc4a51d51"
      },
      "source": [
        "def download_corpus_if_necessary():\n",
        "\n",
        "  if not os.path.exists(corpus_path):\n",
        "       print(\"Descargamos el corpus...\")\n",
        "\n",
        "       # Descargamos el contenido\n",
        "       corpus_string = requests.get(corpus_url).content.decode('utf-8')\n",
        "\n",
        "       # Eliminamos etiquetas HTML\n",
        "       corpus_string = corpus_string.replace(\"<pre>\", \"\")\n",
        "       corpus_string = corpus_string.replace(\"</pre>\", \"\")\n",
        "\n",
        "       # Grabamos en fichero\n",
        "       corpus_file = open(corpus_path, \"w\")\n",
        "       corpus_file.write(corpus_string)\n",
        "       corpus_file.close()\n",
        "\n",
        "       print(\"El corpus se ha grabado en\", corpus_path)\n",
        "  else:\n",
        "       print(\"El corpus ya estaba grabado\")\n",
        "\n",
        "download_corpus_if_necessary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargamos el corpus...\n",
            "El corpus se ha grabado en pg11.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd8FM41ODLCf"
      },
      "source": [
        "## Preprocesado del corpus\n",
        "Solo se preprocesa si no se ha hecho antes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKisSsb_fje",
        "outputId": "f4d9d355-1803-490b-b446-b8b2b5255522"
      },
      "source": [
        "def preprocess_corpus_if_necessary():\n",
        "\n",
        "    if not os.path.exists(preprocessed_corpus_path):\n",
        "        print(\"Preprocesamos el corpus...\")\n",
        "\n",
        "        corpus_file = open(corpus_path, \"r\")\n",
        "        corpus_string = corpus_file.read()\n",
        "\n",
        "        # Generación del vocabulario\n",
        "        print(\"Extraemos las palabras (tokens)...\")\n",
        "        corpus_tokens = word_tokenize(corpus_string)\n",
        "        print(\"Número de tokens:\", len(corpus_tokens))\n",
        "        print(\"Generamos el vocabulario...\")\n",
        "        word_counter = Counter()\n",
        "        word_counter.update(corpus_tokens)\n",
        "        print(\"Longitud del vocabulario antes del corte:\", len(word_counter))\n",
        "        vocabulary = [key for key, value in word_counter.most_common(most_common_words_number)]\n",
        "        print(\"Longitud del vocabulario después del corte:\", len(vocabulary))\n",
        "\n",
        "        # Conversión a índices\n",
        "        print(\"Codificamos a índices...\")\n",
        "        indices = encode_sequence(corpus_tokens, vocabulary)\n",
        "        print(\"Número de índices:\", len(indices))\n",
        "\n",
        "        # Saving.\n",
        "        print(\"Grabamos fichero de preprocesado del corpus...\")\n",
        "        pickle.dump((indices, vocabulary), open(preprocessed_corpus_path, \"wb\"))\n",
        "    else:\n",
        "        print(\"El corpus ya estaba preprocesado\")\n",
        "\n",
        "preprocess_corpus_if_necessary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesamos el corpus...\n",
            "Extraemos las palabras (tokens)...\n",
            "Número de tokens: 36924\n",
            "Generamos el vocabulario...\n",
            "Longitud del vocabulario antes del corte: 3782\n",
            "Longitud del vocabulario después del corte: 3782\n",
            "Codificamos a índices...\n",
            "Número de índices: 36924\n",
            "Grabamos fichero de preprocesado del corpus...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUGvyFcb0u4s"
      },
      "source": [
        "## Generación del dataset\n",
        "Enfocamos la generación del dataset como un **aprendizaje auto-supervisado (self-supervised learning)**:\n",
        "* para generar secuencias de entrada, seleccionamos de forma aleatoria una secuencia de índices (correspondientes a palabras) del corpus\n",
        "* para asignar una etiqueta, escogemos el índice de la palabra siguiente a la secuencia de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z79RLSh50veq"
      },
      "source": [
        "# Parámetros de generación del dataset\n",
        "dataset_size = 50000\n",
        "sequence_length = 30 # longitud de los textos del dataset de entrenamiento\n",
        "\n",
        "def get_dataset(indices):\n",
        "\n",
        "    data_input = []\n",
        "    data_output = []\n",
        "    current_size = 0\n",
        "    bar = progressbar.ProgressBar(max_value=dataset_size)\n",
        "    while current_size < dataset_size:\n",
        "\n",
        "        # seleccionamos de forma aleatoria una secuencia de índices (correspondientes a palabras)\n",
        "        random_index = random.randint(0, len(indices) - (sequence_length + 1))\n",
        "        input_sequence = indices[random_index:random_index + sequence_length]\n",
        "        # la etiqueta de la secuencia aleatoria es el índice de la siguiente palabra\n",
        "        output_label = indices[random_index + sequence_length]\n",
        "\n",
        "        data_input.append(input_sequence)\n",
        "        data_output.append(output_label)\n",
        "\n",
        "        current_size += 1\n",
        "        bar.update(current_size)\n",
        "    bar.finish()\n",
        "\n",
        "    # Transformamos las listas a numpy arrays\n",
        "    data_input = np.array(data_input)\n",
        "    data_output = np.array(data_output)\n",
        "    return (data_input, data_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHOclEylxLOR"
      },
      "source": [
        "## Definición de la red LSTM y entrenamiento\n",
        "Solo se define y entrena si no se ha hecho antes, o si se fuerza con el parámetro train_anyway.\n",
        "\n",
        "Por último, se graba el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "mDbtuIYeA8Rk",
        "outputId": "bea5407d-1a9f-4574-8173-d0f2ffe30147"
      },
      "source": [
        "# Parámetros del modelo y entrenamiento\n",
        "train_anyway = True # fuerza el entrenamiento, aunque se haya realizado antes\n",
        "epochs = 10 # número de interaciones a entrenar\n",
        "batch_size = 128 # tamaño de lote\n",
        "hidden_size = 1000 # número de nodos en la red LSTM\n",
        "model_path = \"lovecraft_model.h5\"\n",
        "\n",
        "def train_neural_network():\n",
        "\n",
        "    if not os.path.exists(model_path) or train_anyway == True:\n",
        "\n",
        "        # Carga de las palabras del corpus y sus índices\n",
        "        indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
        "\n",
        "        # Generación del dataset\n",
        "        print(\"\\nGeneramos el dataset...\")\n",
        "        data_input, data_output = get_dataset(indices)\n",
        "        data_output = to_categorical(data_output, num_classes=len(vocabulary))\n",
        "\n",
        "        # Creación del modelo\n",
        "        print(\"Creamos el modelo...\")\n",
        "        # COMPLETAR CODIGO (debes seguir las indicaciones de la actividad para generar la red LSTM) \n",
        " \n",
        "        model.summary()\n",
        "\n",
        "        # Definimos el resto del modelo: optimizador, función de pérdidas y métrica\n",
        "        # COMPLETAR CODIGO\n",
        "\n",
        "        # Entrenamiento del modelo\n",
        "        print(\"Entrenamos el modelo...\")\n",
        "        # COMPLETAR CODIGO\n",
        "        \n",
        "        # Grabación del modelo\n",
        "        print(\"...y grabamos el modelo\")\n",
        "        model.save(model_path)\n",
        "\n",
        "        plot_history(history)\n",
        "        \n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    print(history.history.keys())\n",
        "\n",
        "    # Gráfica de función de pérdida\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(\"history_loss.png\")\n",
        "    # plt.clf()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    # Gráfica de accuracy.\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(\"history_accuracy.png\")\n",
        "    # plt.clf()\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "        \n",
        "train_neural_network()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25% (12659 of 50000) |####              | Elapsed Time: 0:00:00 ETA:  00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generamos el dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (50000 of 50000) |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creamos el modelo...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b84117c75be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-b84117c75be7>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# COMPLETAR CODIGO (debes seguir las indicaciones de la actividad para generar la red LSTM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Definimos el resto del modelo: optimizador, función de pérdidas y métrica\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzZpOgI3U-T"
      },
      "source": [
        "## Generación de texto\n",
        "\n",
        "La generación de texto se basa en la función **get_index_from_prediction**, que obtiene un índice a partir de una predicción y un parámetro de temperatura "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mry9VsvlW2yU",
        "outputId": "d66ecc0f-ef8b-4616-e8ed-dc3013f040ec"
      },
      "source": [
        "# Parámetros de generación de texto\n",
        "generated_sequence_length = 50 # longitud de los textos generados\n",
        "n_generated_texts = 15 # número de textos a generar\n",
        "\n",
        "def generate_texts(n_generated_texts=10):\n",
        "\n",
        "    print(f\"Generamos {n_generated_texts} textos...\")\n",
        "\n",
        "    # carga del corpus\n",
        "    indices, vocabulary = pickle.load(open(preprocessed_corpus_path, \"rb\"))\n",
        "\n",
        "    # carga del modelo previamente grabado\n",
        "    model = models.load_model(model_path)\n",
        "\n",
        "    # Generamos tantos textos como indique el parámetro n_generated_texts\n",
        "    for _ in range(n_generated_texts):\n",
        "\n",
        "        # Escogemos un parámetro aleatorio \"temperature\" para la predicción\n",
        "        temperature = random.uniform(0.0, 1.0)\n",
        "        print(\"Temperatura:\", temperature)\n",
        "\n",
        "        # Obtenemos una muestra aleatoria a modo de secuencia semilla a partir de la cual se generarán textos\n",
        "        random_index = random.randint(0, len(indices) - (generated_sequence_length))\n",
        "        input_sequence = indices[random_index:random_index + sequence_length]\n",
        "\n",
        "        # Generamos la secuenca de salida repitiendo la predicción\n",
        "        generated_sequence = []\n",
        "        while len(generated_sequence) < generated_sequence_length:\n",
        "            prediction = model.predict(np.expand_dims(input_sequence, axis=0))\n",
        "            predicted_index = get_index_from_prediction(prediction[0], temperature)\n",
        "            generated_sequence.append(predicted_index)\n",
        "            input_sequence = input_sequence[1:]\n",
        "            input_sequence.append(predicted_index)\n",
        "\n",
        "        # Convertimos la secuencia de índices generada en una frase\n",
        "        text = decode_indices(generated_sequence, vocabulary)\n",
        "        print(text)\n",
        "        print(\"\")\n",
        "\n",
        "        \n",
        "def get_index_from_prediction(prediction, temperature=0.0):\n",
        "\n",
        "    # Temperatura cero - usamos argmax.\n",
        "    if temperature == 0.0:\n",
        "        return np.argmax(prediction)\n",
        "\n",
        "    # Temperatura distinta de cero - aplicamos cierta aleatoriedad\n",
        "    else:\n",
        "        prediction = np.asarray(prediction).astype('float64')\n",
        "        prediction = np.log(prediction) / temperature\n",
        "        exp_prediction= np.exp(prediction)\n",
        "        prediction = exp_prediction / np.sum(exp_prediction)\n",
        "        probabilities = np.random.multinomial(1, prediction, 1)\n",
        "        return np.argmax(probabilities)\n",
        "\n",
        "\n",
        "generate_texts(n_generated_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generamos 15 textos...\n",
            "Temperatura: 0.10537251577639051\n",
            "of the whole , thing . It was just quite unbelievably with a very exact about the early house in the house , but the host was not good than any moment by the of the most house . There were no one one of the party now made open\n",
            "\n",
            "Temperatura: 0.1253583293124898\n",
            "even this in this way in this way in the of the old , and that he had been left of the other and scientific earth ? He had not been in the secret room , but this was doubtless by his own by the most bewildering fashion . There\n",
            "\n",
            "Temperatura: 0.7981471394731167\n",
            "my feet was right , and I knew that the edge knew of its I saw in the floor . Now I must be true that himself on the long , with its gigantic - about these instant his first voice was a long , or against the still of\n",
            "\n",
            "Temperatura: 0.8258644772338807\n",
            "ago . It was not time that I was in the sky where was probably escaped ; and there were those who had listen to his place , and when I can do to be at the first day on its eyes , its and with its best and altar\n",
            "\n",
            "Temperatura: 0.1993491499179062\n",
            "to the sky . In the spring a vast corridor to the door . About the walls of a flight and a sense of fear that my uncle 's fear had been the tiny one man had seen of a nameless and beautiful . The sun had not gone ,\n",
            "\n",
            "Temperatura: 0.26641955671332174\n",
            "They have , with his course , though I decided to be on the point of a certain class which were back from them . It was the same man who had seized with his study , and when he had been there for his own nature of his own\n",
            "\n",
            "Temperatura: 0.7909723556469849\n",
            ", and brooding , as the rooms of life and imagination over me . Dim of simple and continued , to For a brief , or further or ; and its , and with the abhorrent the , the and the tales of the which had been . This was\n",
            "\n",
            "Temperatura: 0.0876818058144504\n",
            "and with an incredibly distant case and its lower blocks , the crypt of the dreams were so many of which the earth had been in an entity organism . I think that he was in the sky and bore the long , and seemed to be occupied to exist\n",
            "\n",
            "Temperatura: 0.7839874107186128\n",
            ", and him here that he should never been motion now . The collapse seemed to cluster of my head , for I recalled myself , she would help at his personal his house . That I had long tales of my in his own , and was a long\n",
            "\n",
            "Temperatura: 0.7921992938641115\n",
            "horror in San center . It is a way of this I would not have to in the of a death in the of the grey window , and the whole , with a . , that most , had a blasphemous upon the hill an body ' its name\n",
            "\n",
            "Temperatura: 0.5527837976884012\n",
            "smell . I had had been the of had the leading before ; for it was no to see the bus . I am not to the next day in the old , and was like the next two , and those forms of the words were remnant with itself\n",
            "\n",
            "Temperatura: 0.6100211166470035\n",
            "But Denis told me , though I am not to see on the day 's eyes . The thing it is not a sudden one of his own , and was in a way that they could not but the . This is the great light , or in a\n",
            "\n",
            "Temperatura: 0.4024559395587617\n",
            "that the one he would have to do . It was not that he had been the . I saw that I was myself of a long , that which I can see rows of my fear . I am not nervous the message to me on - I was\n",
            "\n",
            "Temperatura: 0.10194645353723253\n",
            "of human being now who had so many other and and other living and other things in the most city of the earth had taken . On the dead man now he would felt an alien and almost almost ; for the upper parts of the house were too ,\n",
            "\n",
            "Temperatura: 0.40203353492226823\n",
            "the peaked life ; but the upper parts of the house were built . From the table of the was in the street , which in a long period of blue 's presence . As the moon rising watchers and massive and there was still many of them in the\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}